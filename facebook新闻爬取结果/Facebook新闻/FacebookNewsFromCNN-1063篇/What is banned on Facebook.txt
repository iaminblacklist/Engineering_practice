Earlier this week, the murder of a 28-year-old Chicago man was captured live on Facebook. One minute the man was hanging out with friends and livestreaming on Facebook. The next, there are sounds of gunshots and screaming. He was shot Wednesday night, and yet Facebook hasn't pulled the video from its platform, which begs the question: Why not?  According to Facebook, the video doesn't violate its community standards because it doesn't believe the video celebrates violence. It deemed that it falls under a different category: Raising awareness. Instead of removing those types of videos, they're marked with a user warning.  It's a fine line. When an ISIS sympathizer killed a police commander and livestreamed it earlier this week, that video was pulled. Facebook's community standards are broken into eight categories, including attacks on public figures, bullying and harassment, sexual violence and regulated goods. But exactly what it allows and when isn't so cut and dry.  The community standards state that it "aims to find the right balance" to keep people safe, encourage respectful behavior, acknowledge cultural diversity, and empower people to control what they see in their feeds.  Related: Facebook censorship under the microscope But while a video of a murder is OK by Facebook's standards, showing a woman's nipple isn't. The company does, however, "always allow" photos of women showing post-mastectomy scarring or actively engaging in breastfeeding. Although that wasn't always the case. Before 2014, moms had been complaining that their breastfeeding pics were removed. In 2014, Facebook added more context to its policy to allow for these types of non-sexual images. This was thanks in part to a hashtag movement, #FreeThe Nipple, which brought attention to the banning of female nudity on sites like Instagram and Facebook.  Related: The dark side of livestreaming -- Periscoping a rape Facebook does allow nude paintings, sculptures, and other art that depicts nudes -- but sometimes it gets it wrong.  For example, Lee Rowland of the ACLU said in March that she'd had a Facebook post removed because it contained a photo of a nude statue. Facebook's head of policy management Monica Bickert said this didn't actually violate Facebook's policies but was just a mistake.  Similarly, when artist Illma Gore posted a photo of a painting of Donald Trump with a small penis, it was removed. Facebook's community standards allow for nude art that is posted for "educational, humorous, or satirical purposes." According to Facebook, it was also an error and was ultimately restored. Facebook draws a hard line on selling prescription drugs, marijuana, firearms and ammunition-- which it says are prohibited on the platform.  But you are allowed to post photos of, say, yourself smoking marijuana out of a bong. Facebook, like other online communities, relies on a mix of user reports and internal moderation to flag inappropriate or illegal content.  That means Facebook has 1.65 billion people helping enforce its standards -- but what's considered "appropriate" is far from universal.